{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a27c28",
   "metadata": {},
   "source": [
    "# Training Anomaly Detection Model for PetroStream\n",
    "This notebook loads the 3W dataset via KaggleHub and trains an Isolation Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f54867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de520c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on key pressure and temperature sensors\n",
    "FEATURES = [\"P-PDG\", \"P-TPT\", \"T-TPT\", \"P-MON-CKP\", \"T-JUS-CKP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b803df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample_frac: float = 0.1):\n",
    "    \"\"\"\n",
    "    Downloads the 3W dataset using kagglehub, extracts relevant features, and samples for training.\n",
    "    \"\"\"\n",
    "    print(\"Downloading/Locating 3W dataset using kagglehub...\")\n",
    "    data_dir = kagglehub.dataset_download(\"afrniomelo/3w-dataset\")\n",
    "    print(f\"Loading files from {data_dir}...\")\n",
    "    \n",
    "    # Find all simulated parquet files in subdirectories\n",
    "    files = glob.glob(os.path.join(data_dir, \"**\", \"SIMULATED_*.parquet\"), recursive=True)\n",
    "    \n",
    "    # We will just take the first 5 files to keep training extremely fast on the Mac M4\n",
    "    # and fit within memory easily while still proving the pipeline works\n",
    "    selected_files = files[:5] \n",
    "    \n",
    "    dfs = []\n",
    "    for file in selected_files:\n",
    "        try:\n",
    "            df = pd.read_parquet(file)\n",
    "            # Only keep the features we want to train on\n",
    "            df = df[FEATURES].copy()\n",
    "            # Drop any rows that have NaN values in our sensor columns\n",
    "            df.dropna(inplace=True)\n",
    "            dfs.append(df)\n",
    "            print(f\"Loaded {os.path.basename(file)} with {len(df)} valid records.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file}: {e}\")\n",
    "            \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data loaded. Check data path.\")\n",
    "        \n",
    "    combined_df = pd.concat(dfs)\n",
    "    \n",
    "    # Sample down to make training instantaneous\n",
    "    sampled_df = combined_df.sample(frac=sample_frac, random_state=42)\n",
    "    print(f\"Total training samples: {len(sampled_df)}\")\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37727379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df: pd.DataFrame, model_path: str):\n",
    "    \"\"\"\n",
    "    Trains the Isolation Forest anomaly detection model.\n",
    "    \"\"\"\n",
    "    print(\"Training Isolation Forest model...\")\n",
    "    # Isolation Forest is great for detecting rare events (anomalies) in sensor data\n",
    "    # contamination=0.01 means we assume ~1% of the data is actually anomalous\n",
    "    model = IsolationForest(\n",
    "        n_estimators=100,\n",
    "        max_samples=\"auto\",\n",
    "        contamination=0.01,\n",
    "        random_state=42,\n",
    "        n_jobs=-1 # Use all cores on the M4\n",
    "    )\n",
    "    \n",
    "    model.fit(df)\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69326d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"model.joblib\"\n",
    "\n",
    "# 1. Download/Load data via kagglehub\n",
    "training_data = load_data()\n",
    "\n",
    "# 2. Train and save model\n",
    "train_model(training_data, MODEL_PATH)\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(\"1. To deploy, the \"model.joblib\" file should be uploaded to the S3 raw-data bucket.\")\n",
    "print(\"2. The inference Lambda/ECS container will download it to make predictions.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
