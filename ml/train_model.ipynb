{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a27c28",
   "metadata": {},
   "source": [
    "# Training Anomaly Detection Model for PetroStream\n",
    "This notebook loads the 3W dataset via KaggleHub and trains an Random Forest Supervised Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f54867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de520c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on key pressure and temperature sensors\n",
    "FEATURES = [\"P-PDG\", \"P-TPT\", \"T-TPT\", \"P-MON-CKP\", \"T-JUS-CKP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b803df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample_frac: float = 1.0):\n",
    "    \"\"\"\n",
    "    Downloads the 3W dataset using kagglehub, extracts relevant features, and loads ALL valid simulated records for training.\n",
    "    \"\"\"\n",
    "    print(\"Downloading/Locating 3W dataset using kagglehub...\")\n",
    "    data_dir = kagglehub.dataset_download(\"afrniomelo/3w-dataset\")\n",
    "    print(f\"Loading files from {data_dir}...\")\n",
    "    \n",
    "    # Find all simulated parquet files in subdirectories\n",
    "    files = glob.glob(os.path.join(data_dir, \"**\", \"*.parquet\"), recursive=True)\n",
    "    \n",
    "    dfs = []\n",
    "    # Load all 50M+ rows across all files\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(file)\n",
    "            # We must keep the \"class\" column so we can use it to supervise the model\n",
    "            if \"class\" in df.columns:\n",
    "                df = df.dropna(subset=FEATURES + [\"class\"])\n",
    "                # FIX: Remove infinity and clip extreme values so the ML model doesnt crash\n",
    "                df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=FEATURES)\n",
    "                df[FEATURES] = df[FEATURES].clip(lower=-1e30, upper=1e30)\n",
    "                dfs.append(df)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No data loaded. Check data path.\")\n",
    "        \n",
    "    combined_df = pd.concat(dfs)\n",
    "    \n",
    "    # The user explicitly requested ALL data, no row cap\n",
    "    sampled_df = combined_df.sample(frac=sample_frac, random_state=42)\n",
    "        \n",
    "    print(f\"Total valid samples loaded: {len(sampled_df):,}\")\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37727379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def train_model(df: pd.DataFrame, model_path: str):\n",
    "    \"\"\"\n",
    "    Trains a robust Random Forest Classifier using a 70/30 train-test split.\n",
    "    \"\"\"\n",
    "    print(\"Preparing 70:30 Train-Test split...\")\n",
    "    \n",
    "    if \"class\" not in df.columns:\n",
    "        raise ValueError(\"Cannot train Random Forest: missing ground truth \"\"class\"\" column.\")\n",
    "        \n",
    "    # Create the binary target variable\n",
    "    y = [1 if c > 0 else 0 for c in df[\"class\"]]\n",
    "    X = df[FEATURES]\n",
    "    \n",
    "    # 70:30 Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "    print(f\"Training on {len(X_train):,} rows. Validating on {len(X_test):,} rows.\")\n",
    "    \n",
    "    print(\"Training Supervised Random Forest model...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    # Evaluate Accuracy\n",
    "    print(\"\\nEvaluating Model Accuracy on 30% Test Set:\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Normal (0)\", \"Anomaly (1)\"]))\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"\\nModel saved successfully to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69326d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"model.joblib\"\n",
    "\n",
    "# 1. Download/Load data via kagglehub\n",
    "training_data = load_data()\n",
    "\n",
    "# 2. Train and save model\n",
    "train_model(training_data, MODEL_PATH)\n",
    "\n",
    "print(\"\\n--- Next Steps ---\")\n",
    "print(\"1. To deploy, the \\\"model.joblib\\\" file should be uploaded to the S3 raw-data bucket.\")\n",
    "print(\"2. The inference Lambda/ECS container will download it to make predictions.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}